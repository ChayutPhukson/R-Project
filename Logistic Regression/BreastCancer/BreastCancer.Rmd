---
title: "BreastCancer"
author: "Chayut Phukson"
date: "2022-10-20"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: kate
---

# Import Library
```{r}
library(tidyverse)
library(mlbench) # load dataset
```

# Load Data
```{r}
data("BreastCancer")
```

# Review Data
```{r}
head(BreastCancer)
```

```{r}
str(BreastCancer)
```

```{r}
colnames(BreastCancer)
```

# clean data
```{r}
# delete column Id
BreastCancer$Id <- NULL
head(BreastCancer)
```

```{r}
# check NA
table(is.na(BreastCancer))
```

```{r}
# delete NA
BreastCancer <- na.omit(BreastCancer)

# cheack again
table(is.na(BreastCancer))
```

# Split Data
Divide the data into train 80% and test 20%
```{r}
set.seed(1)
n <- nrow(BreastCancer)
train_id <- sample(1:n, size = 0.8*n)
train_BreastCancer <- BreastCancer[train_id, ]
test_BreastCancer <- BreastCancer[-train_id, ]
```

# Train Model
```{r}
# Train logistic regression
Logistic_Model <- glm(Class ~ ., data = train_BreastCancer, family = "binomial")
```

prediction by

if probability >= 0.5 is positive (malignant, T)

but probability < 0.5 is negative (benign, F)
```{r}
# Predict and evaluate the Train data
p1 <- predict(Logistic_Model, type = "response")

# prediction by
# if probability >= 0.5 (malignant) is positive 
# but probability < 0.5 is negative (benign)
prediction1 <- ifelse(p1 >= 0.5, "malignant", "benign")
```

# Train Accuracy 
```{r}
Train_Accuracy <- mean(prediction1 == train_BreastCancer$Class)
paste("Train Accuracy:", Train_Accuracy * 100, "%")
```

# Test Model
```{r}
p2 <- predict(Logistic_Model, newdata = test_BreastCancer, type = "response")
prediction2 <- ifelse(p2 >= 0.5, "malignant", "benign")
```

# Test Accuracy 
```{r}
Test_Accuracy <- mean(prediction2 == test_BreastCancer$Class)
paste("Test Accuracy:", Test_Accuracy * 100, "%")
```

# Summarize 
Train Accuracy = 100 % > Test Accuracy = 91.24 %

We can conclude the model have overfitting because the accuracy of the Train Model is much more than the accuracy of the Test Model.

So we will use Regularization method to reduce overfitting

# Regularization
```{r}
# load library
library(caret) # confusion matrix
library(glmnet)
```

# Train with 5-Fold CV (k = 5 is popular)
k = 5 and k = 10 are popular

# Train Regularization Model
```{r}
# Train Model
ctrl <- trainControl(method = "cv", number = 5, search = "random", verboseIter = T) # we use random search
set.seed(1)
Regularization_Model <- train(Class ~ ., 
                           data = train_BreastCancer, 
                           method = "glmnet",
                           metric = "Accuracy",
                           tuneLength = 15, # we'll try 15 alpha and 15 lambda values
                           trControl = ctrl,
                           family = "binomial")
Regularization_Model
```

# Train Accuracy Regularization Model
```{r}
p3 <- predict(Regularization_Model, type = "raw")
Train_Accuracy_Regularization <- mean(p3 == train_BreastCancer$Class)
paste("Train Accuracy:", Train_Accuracy_Regularization * 100, "%")
```

# Test Accuracy Regularization Model
```{r}
# Test Model
p4 <- predict(Regularization_Model, newdata = test_BreastCancer, type = "raw")
Test_Accuracy_Regularization <- mean(p4 == test_BreastCancer$Class)
paste("Test Accuracy:", Test_Accuracy_Regularization * 100, "%")
```

# confusion matrix
```{r}
# train model
table(p3, train_BreastCancer$Class, dnn = c("prediction", "actual"))
```

```{r}
# train model
confusionMatrix(data = factor(p3, 
                              levels = c("benign", "malignant"), 
                              labels = c("benign", "malignant")), 
                reference = factor(train_BreastCancer$Class, 
                                   levels = c("benign" , "malignant"), 
                                   labels = c("benign" , "malignant")))
```

```{r}
# test model
table(p4, test_BreastCancer$Class, dnn = c("prediction", "actual"))
```

```{r}
# test model
confusionMatrix(data = factor(p4, 
                              levels = c("benign", "malignant"), 
                              labels = c("benign", "malignant")), 
                reference = factor(test_BreastCancer$Class, 
                                   levels = c("benign" , "malignant"), 
                                   labels = c("benign" , "malignant")))
```

# Final Model
```{r}
# exp(coef(Regularization_Model$finalModel))
```

ref:

"https://datarockie.com/blog/logistic-regression-r/"

"https://www.youtube.com/watch?v=nFJgel5Cv0E&list=PLoTScYm9O0GGat89RT9NMjW7sqFz84XSk&index=6"



